<!DOCTYPE html>
<html lang="English">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>1. Overview | OpenNCC Docs</title>
    <meta name="generator" content="VuePress 1.6.0">
    
    <meta name="description" content="Open AI Camera">
    <link rel="preload" href="/openncc/assets/css/0.styles.a796fc16.css" as="style"><link rel="preload" href="/openncc/assets/js/app.eaab4845.js" as="script"><link rel="preload" href="/openncc/assets/js/2.b37cbe72.js" as="script"><link rel="preload" href="/openncc/assets/js/14.32ae3710.js" as="script"><link rel="prefetch" href="/openncc/assets/js/10.a14724c4.js"><link rel="prefetch" href="/openncc/assets/js/11.9a74d827.js"><link rel="prefetch" href="/openncc/assets/js/12.e7570781.js"><link rel="prefetch" href="/openncc/assets/js/13.cbbee07c.js"><link rel="prefetch" href="/openncc/assets/js/15.65758b8b.js"><link rel="prefetch" href="/openncc/assets/js/16.99e35a12.js"><link rel="prefetch" href="/openncc/assets/js/17.a2b92fb7.js"><link rel="prefetch" href="/openncc/assets/js/18.1f7d3d64.js"><link rel="prefetch" href="/openncc/assets/js/19.46e456a1.js"><link rel="prefetch" href="/openncc/assets/js/20.4a50278a.js"><link rel="prefetch" href="/openncc/assets/js/21.2aa52ecc.js"><link rel="prefetch" href="/openncc/assets/js/22.f9bd4122.js"><link rel="prefetch" href="/openncc/assets/js/23.aeb9df3b.js"><link rel="prefetch" href="/openncc/assets/js/24.7f970d67.js"><link rel="prefetch" href="/openncc/assets/js/25.231d7036.js"><link rel="prefetch" href="/openncc/assets/js/26.c1bd6cb5.js"><link rel="prefetch" href="/openncc/assets/js/3.7c13d29b.js"><link rel="prefetch" href="/openncc/assets/js/4.caa66714.js"><link rel="prefetch" href="/openncc/assets/js/5.b0610f3e.js"><link rel="prefetch" href="/openncc/assets/js/6.761b8afb.js"><link rel="prefetch" href="/openncc/assets/js/7.67c0f35b.js"><link rel="prefetch" href="/openncc/assets/js/8.310d904a.js"><link rel="prefetch" href="/openncc/assets/js/9.1bc7a4c2.js">
    <link rel="stylesheet" href="/openncc/assets/css/0.styles.a796fc16.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/openncc/" class="home-link router-link-active"><!----> <span class="site-name">OpenNCC Docs</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://www.openncc.com" target="_blank" rel="noopener noreferrer" class="nav-link external">
  OpenNCC
  <svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://github.com/EyecloudAi/openncc" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://docs.openvinotoolkit.org/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  OpenVINO
  <svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Select language" class="dropdown-title"><span class="title">Languages</span> <span class="arrow down"></span></button> <button type="button" aria-label="Select language" class="mobile-dropdown-title"><span class="title">Languages</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/openncc/Software_Manual.html" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  English
</a></li><li class="dropdown-item"><!----> <a href="/openncc/zh/" class="nav-link">
  中文
</a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://www.openncc.com" target="_blank" rel="noopener noreferrer" class="nav-link external">
  OpenNCC
  <svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://github.com/EyecloudAi/openncc" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://docs.openvinotoolkit.org/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  OpenVINO
  <svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Select language" class="dropdown-title"><span class="title">Languages</span> <span class="arrow down"></span></button> <button type="button" aria-label="Select language" class="mobile-dropdown-title"><span class="title">Languages</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/openncc/Software_Manual.html" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  English
</a></li><li class="dropdown-item"><!----> <a href="/openncc/zh/" class="nav-link">
  中文
</a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/openncc/DKIntroduction.html" class="sidebar-link">Introduction of OpenNCC</a></li><li><a href="/openncc/DKHardwareManual.html" class="sidebar-link">OpenNCC Hardware Manual</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/openncc/DKHardwareManual.html#overview-of-the-openncc-hardware" class="sidebar-link">Overview of the OpenNCC Hardware</a></li><li class="sidebar-sub-header"><a href="/openncc/DKHardwareManual.html#hardware-spec" class="sidebar-link">Hardware Spec.</a></li></ul></li><li><a href="/openncc/GettingStart.html" class="sidebar-link">OpenNCC Getting Start</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/openncc/GettingStart.html#applicable-specifications" class="sidebar-link">Applicable specifications</a></li><li class="sidebar-sub-header"><a href="/openncc/GettingStart.html#operation-steps" class="sidebar-link">Operation steps</a></li><li class="sidebar-sub-header"><a href="/openncc/GettingStart.html#demo-models-of-openncc" class="sidebar-link">Demo models of OpenNCC</a></li></ul></li><li><a href="/openncc/nccview.html" class="sidebar-link">OpenNCC View's Manual</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/openncc/nccview.html#_1-overview" class="sidebar-link">1. Overview</a></li><li class="sidebar-sub-header"><a href="/openncc/nccview.html#_2-support-platform" class="sidebar-link">2.Support platform</a></li><li class="sidebar-sub-header"><a href="/openncc/nccview.html#_3-start-openncc-view" class="sidebar-link">3.Start OpenNCC View</a></li><li class="sidebar-sub-header"><a href="/openncc/nccview.html#_4-getting-start" class="sidebar-link">4.Getting start</a></li><li class="sidebar-sub-header"><a href="/openncc/nccview.html#_5-model-analysis" class="sidebar-link">5.Model analysis</a></li><li class="sidebar-sub-header"><a href="/openncc/nccview.html#_6-functions-detail" class="sidebar-link">6.Functions detail</a></li></ul></li><li><a href="/openncc/Software_Manual.html" aria-current="page" class="active sidebar-link">Software User's Manual</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/openncc/Software_Manual.html#_1-overview" class="sidebar-link">1. Overview</a></li><li class="sidebar-sub-header"><a href="/openncc/Software_Manual.html#_2-cdk-introduction" class="sidebar-link">2. CDK Introduction</a></li><li class="sidebar-sub-header"><a href="/openncc/Software_Manual.html#_3-openvino-installation-and-getting-start" class="sidebar-link">3. OpenVINO installation and getting start</a></li><li class="sidebar-sub-header"><a href="/openncc/Software_Manual.html#_4-openncc-operating-mechanism" class="sidebar-link">4. OpenNCC operating mechanism</a></li></ul></li><li><a href="/openncc/Openncc_sdk_api.html" class="sidebar-link">SDK API</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/openncc/Openncc_sdk_api.html#openncc-sdk-api-2-0-x-interface-documentation" class="sidebar-link">OpenNCC SDK API 2.0.x Interface Documentation</a></li></ul></li><li><a href="/openncc/openvino_install.html" class="sidebar-link">How to install OpenVINO</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/openncc/openvino_install.html#download-and-install-openvino" class="sidebar-link">Download and Install OpenVINO</a></li></ul></li><li><a href="/openncc/DownloadLink.html" class="sidebar-link">Download Released Packets</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/openncc/DownloadLink.html#download-links" class="sidebar-link">Download Links</a></li></ul></li><li><a href="/openncc/FAQ.html" class="sidebar-link">FAQ</a><ul class="sidebar-sub-headers"></ul></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="_1-overview"><a href="#_1-overview" class="header-anchor">#</a> 1. Overview</h2> <p>This document introduces the basic concepts of OpenNCC deployment, OpenNCC CDK and OpenVINO, and the method of using OpenNCC CDK to develop and deploy OpenNCC DK independent operation mode and mixed mode with OpenVINO.</p> <h3 id="_1-1-support-platform"><a href="#_1-1-support-platform" class="header-anchor">#</a> 1.1 Support platform</h3> <p>Hardware：OpenNCC DK R1、OpenNCC Knight、OpenNCC USB<br>
 PC OS：Ubuntu16.04, Ubuntu18.04, Raspberry Pi OS, ARM Linux (Need to provide toolchain cross compilation)<br>
 Support language: C/C++、Python3.5、Python3.7<br>
OpenVINO: 2019.R1.144</p> <h3 id="_1-2-customer-support-center"><a href="#_1-2-customer-support-center" class="header-anchor">#</a> 1.2 Customer Support Center</h3> <p>Please visit  https://www.openncc.com for more updates.</p> <h2 id="_2-cdk-introduction"><a href="#_2-cdk-introduction" class="header-anchor">#</a> 2. CDK Introduction</h2> <p>OpenNCC CDK is a set of toolkits specifically developed for OpenNCC cameras for rapid deployment of deep learning in OpenNCC devices.</p> <h3 id="_2-1-cdk-development-package-directory-structure"><a href="#_2-1-cdk-development-package-directory-structure" class="header-anchor">#</a> 2.1 CDK development package directory structure</h3> <table><thead><tr><th>Contents</th> <th>abstract</th></tr></thead> <tbody><tr><td>docs</td> <td>OpenNCC Offline documentation</td></tr> <tr><td>View/Linux</td> <td>OpenView for Linux</td></tr> <tr><td>View/Windows</td> <td>OpenView for Windows</td></tr> <tr><td>Public/Library/For_C&amp;C++/Linux</td> <td>C/C++ OpenNCC CDK static library on Linux and VPU USB bootloader</td></tr> <tr><td>Public/Library/For_C&amp;C++/Windows</td> <td>C/C++ OpenNCC CDK static library on Windows and VPU USB bootloader</td></tr> <tr><td>Public/Library/For_Python</td> <td>Python version OpenNCC CDK package, and demo program</td></tr> <tr><td>Public/Library/Raspberry</td> <td>Raspberry  version OpenNCC CDK package</td></tr> <tr><td>Sample/bin</td> <td>The Firmwares and bootloader for the AI Camera</td></tr> <tr><td>Sample/Demo/work with OpenVINO/human_pose_estimation_demo</td> <td>Human pose demo,and the decoder and display running under openvino</td></tr> <tr><td>Sample/Demo/work with OpenVINO/interactive_face_detection_demo</td> <td>Face dectection and attributes demo,and the decoder and display running under openvino</td></tr> <tr><td>Samples/How_to/Capture video</td> <td>Sample program, use CDK library to get video stream</td></tr> <tr><td>Samples/How_to/load a model</td> <td>Sample program, using the CDK library to load a deep learning model in Blob format</td></tr> <tr><td>Samples/How_to/work_with_multiple_models</td> <td>Sample program, using the CDK library to load two deep learning models in Blob format</td></tr> <tr><td>Tools/myriad_compiler</td> <td>IR file conversion Blob file tool</td></tr> <tr><td>Tools/deployment</td> <td>Kit deployment script</td></tr></tbody></table> <h2 id="_3-openvino-installation-and-getting-start"><a href="#_3-openvino-installation-and-getting-start" class="header-anchor">#</a> 3. OpenVINO installation and getting start</h2> <p>   To deploy a deep learning model on end-point target devices, you need to optimize and convert a trained model to the VPU characteristics to achieve higher operating performance. OpenNCC is compatible with OpenVINO's tool set and model format, and needs to rely on Intel OpenVINO's model optimizer to complete model optimization and conversion into Blob format. When using OpenNCC CDK, you need to install OpenVINO as follows:
If you need to convert the trained model yourself, you need to install OpenVINO to run the model optimizer.
When OpenVINO runs in a mixed mode with the OpenVINO inference engine, it also needs OpenVINO support.</p> <h3 id="_3-1-download-and-install-openvino"><a href="#_3-1-download-and-install-openvino" class="header-anchor">#</a> 3.1 Download and install OpenVINO</h3> <p>  OpenNCC currently supports OpenVINO version: 2019R1.144, OpenVINO installation reference <a href="/openncc/openvino_install.html">here</a></p> <h3 id="_3-2-intel-free-model-download"><a href="#_3-2-intel-free-model-download" class="header-anchor">#</a> 3.2 Intel Free model download</h3> <p>  OpenNCC supports OpenVINO models, Intel has a large number of free trained models for learning reference and testing. After we have installed OpenVINO, we can use the Intel download tool to download the model collection. Model download tool path: <code>openvino/deployment_tools/tools/model_downloader/downloader.py</code>, common commands are as follows:</p> <ul><li>View all downloadable models：./downloader.py --print</li> <li>Download the specified model：./downloader.py --name *</li></ul> <p>For example, download a face detection model ：<code>./downloader.py --name face-detection-adas-0001-fp16</code><br> <img src="/openncc/docimg/sw_figure1.png" alt="Figure-1"></p> <h3 id="_3-3-model-optimization-and-format-conversion"><a href="#_3-3-model-optimization-and-format-conversion" class="header-anchor">#</a> 3.3 Model optimization and format conversion</h3> <p> When we need to deploy a trained model to OpenNCC, we need to optimize and transform the model. After installing OpenVINO, you can use the model optimization tool: <code>/opt/intel/openvino/deployment_tools/model_optimizer/mo.py</code> to optimize the model. For specific documents, see the official Intel documents: <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html" target="_blank" rel="noopener noreferrer">Model Optimizer Developer Guide<svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.<br>
 After the model optimization is completed, the model needs to be converted to the Blob format before it can be deployed on OpenNCC. In the OpenVINO installation directory: <code>/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64myriad_compile</code> tool, the method of use is as follows:
Enter from the command line terminal：<code>./myriad_compile -m input_xxx-fp16.xml  -o output_xxx.blob  -VPU_PLATFORM VPU_2480 -VPU_NUMBER_OF_SHAVES  8  -VPU_NUMBER_OF_CMX_SLICES 8</code><br>
  After the format conversion is completed, the model can be deployed on OpenNCC, refer to: <code>ncc_cdk/Samples/How_to/load a model</code>, or use the OpenNCC View interface program to add the model to deploy and test it.</p> <h2 id="_4-openncc-operating-mechanism"><a href="#_4-openncc-operating-mechanism" class="header-anchor">#</a> 4. OpenNCC operating mechanism</h2> <p>  From a model training environment to embedded deployment, it is a very important task, which requires mastering the framework of deep learning, such as commonly used: Caffe*, TensorFlow*, MXNet*, Kaldi*, etc.In addition, it is very important to master the deployed embedded platform. You need to understand the platform performance, system architecture characteristics, and then combine the platform characteristics to optimize the training model framework, and finally tune, transplant, and deploy to the embedded platform.<br>
  OpenNCC focuses on the rapid deployment of deep learning models, is compatible with Intel OpenVINO tools, and for embedded graphics and image application scenarios, it has completed the integration of different resolution sensors from 2MP to 20MP on end-point target devices, and the end-point target devices has realized the deployment of professional-level ISP. OpenVINO optimized converted model files can be dynamically downloaded to the end-point OpenNCC camera to achieve rapid deployment of deep learning models.OpenNCC has designed independent working mode, mixed development mode and co-processing compute stick mode to adapt to different work application scenarios.</p> <h3 id="_4-1-openncc-standalone-mode"><a href="#_4-1-openncc-standalone-mode" class="header-anchor">#</a> 4.1 OpenNCC standalone mode</h3> <p> In the independent mode, OpenNCC independently runs a deep learning model, and feeds back the inference results to the user through the OpenNCC CDK API.
The application deployment process is as follows:<br> <img src="/openncc/docimg/sw_figure2.png" alt="Figure-2"><br>
 According to the OpenVINO documentation, for a specific training framework <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_MO_DG_prepare_model_Config_Model_Optimizer.html" target="_blank" rel="noopener noreferrer">Configure Model Optimizer<svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><br>
 Run <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_IE_DG_Introduction.html#MO" target="_blank" rel="noopener noreferrer">Model Optimizer<svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> to produce an optimized Intermediate Representation (IR) of the model based on the trained network topology, weights and biases values, and other optional parameters.
The IR is a pair of files that describe the whole model:</p> <ul><li>.xml: The topology file - an XML file that describes the network topology</li> <li>.bin: The trained data file - a .bin file that contains the weights and biases binary data<br>
Then run myriad_compile to generate a BLOB file from the IR file.<br>
To integrate the BLOB model file generated after optimization using OpenNCC CDK, see the demo program of <code>Samples/How_to/Load a model</code> under CDK.<br>
  OpenNCC View is an application demonstration program with an operating interface integrated with OpenNCC CDK. You can also use OpenView to deploy models and obtain test results. Refer to OpenNCC View Guide Because different depth models have differentiated inference output results, if users cannot find a suitable post-processing analytical model under the CDK, they need to refer to <code>ncc_cdk/Samples/How_to/load a model</code> and write post-processing code in combination with their own application scenarios.</li></ul> <h4 id="_4-1-1-secondary-model-operation-support"><a href="#_4-1-1-secondary-model-operation-support" class="header-anchor">#</a> 4.1.1 Secondary model operation support</h4> <p>Considering the end-to-side computing capability, at present, CDK multi-level models support cascading of two-level models, as shown in the following figure:<br> <img src="/openncc/docimg/zh/SoftManualF10.jpg" alt="F"><br>
The first level model must be a target detection or classification model, and the output is defined as follows:<br> <img src="/openncc/docimg/zh/SoftManualF11.jpg" alt="F"><br>
process：<br>
1）After pre CV [1], the original image scale is converted to the input size of the first level model, and the corresponding format conversion is performed. Then the first level model reasoning calculation is performed, and the first level reasoning result is output to pre CV [2].<br>
2) The pre CV [2] module analyzes the reasoning results of the first level model, and takes the qualified label and conf detection target according to the coordinate starting point (x)_ min, y_ Min), the end point (x)_ max,y_ Max) from the original graph's Cross and scale are converted to the input size of the secondary model, and the corresponding format conversion is performed to enter the second level model reasoning.<br>
3）Finally, the reasoning results of the first level model and all the second level models are packaged and output together.<br>
Model output analysis (parameter configuration in the figure is: valid label: 2,3, conf = 0.8)<br> <img src="/openncc/docimg/zh/SoftManualF12.png" alt="F"></p> <p>Sample：<code>Samples/How_to/work_with_multiple_models</code>,the first level model is vehicle and license plate detection, the second level model is license plate detection, and the effective label is set to 2<br>
Based on the detection results of the first stage, the detection coordinates of the first stage are adjusted appropriately, which is conducive to the identification of:<br>
*Fine tuning the starting point to the left and up（startXAdj，startYAdj ）<br>
*Bottom right down fine adjustment（endXAdj，endYAdj）<br>
cnn2PrmSet.startXAdj  = -5;<br>
cnn2PrmSet.startYAdj  = -5;<br>
cnn2PrmSet.endXAdj   = 5;<br>
cnn2PrmSet.endYAdj   = 5;</p> <h3 id="_4-2-openncc-mixed-mode"><a href="#_4-2-openncc-mixed-mode" class="header-anchor">#</a> 4.2 OpenNCC mixed mode</h3> <p> When it is necessary to solve some complex application scenarios, multiple network model combination processing is required, OpenNCC end-point computing performance cannot be met, or the end-side processing needs to be concentrated on the edge side for post-processing, system expansion is often required. Run the models with high real-time requirements on the OpenNCC end-point, and the other models on the post-processing edge machine or cloud.<br>
 As shown in the figure, Model-1 runs on the OpenNCC end-point  to complete the pre-processing of the video stream. OpenNNC returns the results of the first-level processing model to the user application. Model-1 and Model-2 fully run under the OpenVINO inference engine to implement subsequent processing.<br> <img src="/openncc/docimg/sw_figure3.png" alt="Figure-3"><br>
 In ncc_cdk/Samples/Demo/work with OpenVINO demonstrated how to combine OpenNCC and OpenVINO on Host PC to implement a distributed AI system.</p> <h3 id="_4-3-co-processing-compute-stick-mode"><a href="#_4-3-co-processing-compute-stick-mode" class="header-anchor">#</a> 4.3 Co-processing compute stick mode</h3> <p> OpenNCC's co-processing mode is similar to Intel NCS2. In this mode of operation, OpenNCC's vision sensor does not work, and users can use OpenNCC alone to achieve full compatibility with the OpenVINO environment. The typical deep learning model deployment process of OpenVINO is as follows:<br> <img src="/openncc/docimg/sw_figure4.png" alt="Figure-4"><br> <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_MO_DG_prepare_model_Config_Model_Optimizer.html" target="_blank" rel="noopener noreferrer">Configure Model Optimizer<svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> for specific training framework according to OpenVINO documentation.<br>
 Run Model Optimizer to produce an optimized Intermediate Representation (IR) of the model based on the trained network topology, weights and biases values, and other optional parameters.<br>
 Download the optimized IR file to OpenNCC to run the Inference Engine. For details, refer to OpenVINO documents: <a href="https://docs.openvinotoolkit.org/2019_R1.1/_inference_engine_samples_validation_app_README.html" target="_blank" rel="noopener noreferrer">Inference Engine validation application<svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and <a href="https://docs.openvinotoolkit.org/2019_R1.1/_docs_IE_DG_Samples_Overview.html" target="_blank" rel="noopener noreferrer">sample applications.<svg xmlns="http://www.w3.org/2000/svg" aria-labelledby="outbound-link-title" role="img" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><title id="outbound-link-title">(opens new window)</title> <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><br>
 Copy Public/Firmwares/MvNCAPI-ma2480.mvcmd and replace openvino/inference_engine/lib/intel64/MvNCAPI-ma2480.mvcmd in the openvino installation directory.(Remarks: MvNCAPI-ma2480.mvcmd in the openvino installation directory must be backed up before replacement. This file needs to be restored when using NCS2 inference)</p> <h3 id="_4-4-difference-between-independent-mode-and-co-processing-mode"><a href="#_4-4-difference-between-independent-mode-and-co-processing-mode" class="header-anchor">#</a> 4.4 Difference between independent mode and co-processing mode</h3> <p> The right side of the figure below is the independent mode of OpenNCC, and the left side is the co-processing mode of OpenNCC (similar to Intel NCS2).<br> <img src="/openncc/docimg/sw_figure5.png'" alt="Figure-5">)<br>
 When we need to deploy a vision-based deep learning model, first we need to obtain a high-quality video stream, then run the inference engine to calculate the input image data, and finally output the result. For the co-processing mode on the left, we need an OpenNCC DK or Intel NCS2 implements end-to-side reasoning. At the same time, we need to obtain a video stream from a camera and send the video frame to OpenNCC DK via USB. In the independent mode on the right, no additional camera is needed to obtain the video stream. We only need to download the model to OpenNCC to obtain the deduction results.<br>
Refer to OpenVINO official website:https://docs.openvinotoolkit.org/</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/openncc/nccview.html" class="prev">
        OpenNCC View's Manual
      </a></span> <span class="next"><a href="/openncc/Openncc_sdk_api.html">
        SDK API
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/openncc/assets/js/app.eaab4845.js" defer></script><script src="/openncc/assets/js/2.b37cbe72.js" defer></script><script src="/openncc/assets/js/14.32ae3710.js" defer></script>
  </body>
</html>
